{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sample dataset with both sentiment and intent labels\n",
        "texts = [\n",
        "    \"I have heard good things about this from my friend. I want to buy it!\",  # (Positive, Purchase Intent)\n",
        "    \"This is the worst thing I've ever bought.\",  # (Negative, Complaint)\n",
        "    \"Excellent quality, highly recommended you to buy it\",  # (Positive, Purchase Intent)\n",
        "    \"I am not satisfied at all, it has been terrible experience.\",  # (Negative, Complaint)\n",
        "    \"I am very happy with this product in past would like to purchase again!\",  # (Positive, Purchase Intent)\n",
        "    \"I hate this product, completely useless.\",  # (Negative, Complaint)\n",
        "    \"Great value for money!\",  # (Positive, Inquiry)\n",
        "    \"I am Disappointed, would not buy this product again.\"  # (Negative, Inquiry)\n",
        "]"
      ],
      "metadata": {
        "id": "SGZVJgX8x8j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK 1: Sentence Transformer Implementation**"
      ],
      "metadata": {
        "id": "2y8hg0PI0OB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK\n",
        "embeddings = np.array(model.encode(texts))\n",
        "\n",
        "len(embeddings[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjPB97Au0K2a",
        "outputId": "590a3b00-9931-4b7d-80df-5ef7a4a2b004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLDiL37803lK",
        "outputId": "34d732fd-c84b-4cd5-d5ec-107ea1b5e1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sentence become embedding becomes length of size 384"
      ],
      "metadata": {
        "id": "adtwr1DR0oIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK 2 Multi Task learning Expansion**"
      ],
      "metadata": {
        "id": "bC6n0zwE1BLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for two tasks\n",
        "sentiment_labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "intent_labels = [\"purchase\", \"complaint\", \"purchase\", \"complaint\", \"purchase\", \"complaint\", \"inquiry\", \"inquiry\"]\n",
        "\n",
        "# Convert text to embeddings\n",
        "embeddings = np.array(model.encode(texts))\n",
        "\n",
        "# Encode intent labels as integers\n",
        "intent_encoder = LabelEncoder()\n",
        "intent_labels_encoded = intent_encoder.fit_transform(intent_labels)  # Converts to 0, 1, 2 classes\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train_sentiment, y_test_sentiment, y_train_intent, y_test_intent = train_test_split(\n",
        "    embeddings, sentiment_labels, intent_labels_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define input layer\n",
        "input_layer = keras.layers.Input(shape=(embeddings.shape[1],))\n",
        "\n",
        "# Shared hidden layers\n",
        "hidden = keras.layers.Dense(128, activation=\"relu\")(input_layer)\n",
        "hidden = keras.layers.Dropout(0.3)(hidden)\n",
        "hidden = keras.layers.Dense(64, activation=\"relu\")(hidden)\n",
        "\n",
        "# Sentiment classification head\n",
        "sentiment_head = keras.layers.Dense(1, activation=\"sigmoid\", name=\"sentiment_output\")(hidden)\n",
        "\n",
        "# Intent classification head\n",
        "intent_head = keras.layers.Dense(len(set(intent_labels_encoded)), activation=\"softmax\", name=\"intent_output\")(hidden)\n",
        "\n",
        "# Define multi-output model\n",
        "multi_output_model = keras.Model(inputs=input_layer, outputs=[sentiment_head, intent_head])\n",
        "\n",
        "# Compile the model with separate loss functions\n",
        "multi_output_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss={\n",
        "        \"sentiment_output\": \"binary_crossentropy\",\n",
        "        \"intent_output\": \"sparse_categorical_crossentropy\",\n",
        "    },\n",
        "    metrics={\n",
        "        \"sentiment_output\": \"accuracy\",\n",
        "        \"intent_output\": \"accuracy\",\n",
        "    },\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-d3CgjcbzqgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To support multi-task learning, the architecture was modified to have a shared hidden layers followed by two separate output heads—one for sentiment classification and another for intent classification.\n",
        "\n",
        "The Sentence Transformer (all-MiniLM-L6-v2) generates vector embeddings from input text.\n",
        "A shared feedforward neural network processes these embeddings.\n",
        "This shared hidden representation allows both tasks to leverage common linguistic features, improving efficiency and generalization.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dTCua0aD1xyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3  \n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "1.   If the entire network is frozen network does not learn anything only inference is possible so this may not be viable option\n",
        "2.   When only transformer architecture is frozen we depend on transformer to generate the fixed embeddings which are then passed to 2 heads and only task specific heads are trained.\n",
        "3.   If one of the task-specific heads is frozen, that particular task does not learn anything.\n",
        "\n",
        "In our case we have frozen the tranformer layers and we are training the task specific heads\n",
        "\n",
        "\n",
        "Choice of Pre-Trained Model\n",
        "\n",
        "1.   Choice of Pre-Trained Model\n",
        "\n",
        "General NLP: all-MiniLM-L6-v2 (efficient, strong embeddings).\n",
        "Domain-Specific: SciBERT for science , BioBERT for Biology, BloombergGPT for finance\n",
        "2.   Layers to Freeze/Unfreeze\n",
        "\n",
        "Step 1: Freeze Transformer Backbone (preserves pre-trained knowledge, ideal for small data).\n",
        "Step 2: Train Task-Specific Heads (adapts to new tasks).\n",
        "Step 3: Unfreeze Top Transformer Layers (if more adaptation is needed).\n",
        "\n",
        "3.   Recommendations\n",
        "\n",
        "Small dataset → Freeze backbone, train heads.\n",
        "Large dataset → Gradually unfreeze top layers.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Ww1htu4U40YB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnhyZy5m9_kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 4 - Training"
      ],
      "metadata": {
        "id": "Ta_OXFRo9g_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "multi_output_model.fit(\n",
        "    X_train, {\"sentiment_output\": np.array(y_train_sentiment), \"intent_output\": np.array(y_train_intent)},\n",
        "    epochs=10, batch_size=4, validation_data=(X_test, {\"sentiment_output\": np.array(y_test_sentiment), \"intent_output\": np.array(y_test_intent)})\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = multi_output_model.evaluate(X_test, {\"sentiment_output\": np.array(y_test_sentiment), \"intent_output\": np.array(y_test_intent)})\n",
        "print(f\"Test Sentiment Accuracy: {test_results[3]:.2f}, Test Intent Accuracy: {test_results[4]:.2f}\")\n",
        "\n",
        "# Predict on new sample\n",
        "new_text = [\"I'm looking for a great laptop deal.\"]\n",
        "new_embedding = np.array(sbert_model.encode(new_text))\n",
        "\n",
        "sentiment_pred, intent_pred = multi_output_model.predict(new_embedding)\n",
        "sentiment_label = \"Positive\" if sentiment_pred[0][0] > 0.5 else \"Negative\"\n",
        "intent_label = intent_encoder.inverse_transform([np.argmax(intent_pred[0])])[0]\n",
        "\n",
        "print(f\"Predicted Sentiment: {sentiment_label}, Predicted Intent: {intent_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-gy-dgj1wrQ",
        "outputId": "939ced9b-2431-4e9a-d517-af57a8f14acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 1.3571 - sentiment_output_loss: 0.5371 - intent_output_loss: 0.8200 - sentiment_output_accuracy: 0.6667 - intent_output_accuracy: 0.8333 - val_loss: 2.0306 - val_sentiment_output_loss: 0.7553 - val_intent_output_loss: 1.2753 - val_sentiment_output_accuracy: 0.0000e+00 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 1.2918 - sentiment_output_loss: 0.4919 - intent_output_loss: 0.7999 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 0.6667 - val_loss: 2.0577 - val_sentiment_output_loss: 0.7599 - val_intent_output_loss: 1.2978 - val_sentiment_output_accuracy: 0.0000e+00 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 1.1799 - sentiment_output_loss: 0.4563 - intent_output_loss: 0.7236 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 0.8333 - val_loss: 2.0879 - val_sentiment_output_loss: 0.7657 - val_intent_output_loss: 1.3222 - val_sentiment_output_accuracy: 0.0000e+00 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 1.1284 - sentiment_output_loss: 0.4116 - intent_output_loss: 0.7168 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 1.0000 - val_loss: 2.1145 - val_sentiment_output_loss: 0.7661 - val_intent_output_loss: 1.3484 - val_sentiment_output_accuracy: 0.0000e+00 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 1.0944 - sentiment_output_loss: 0.4155 - intent_output_loss: 0.6788 - sentiment_output_accuracy: 0.8333 - intent_output_accuracy: 1.0000 - val_loss: 2.1368 - val_sentiment_output_loss: 0.7645 - val_intent_output_loss: 1.3723 - val_sentiment_output_accuracy: 0.0000e+00 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.0069 - sentiment_output_loss: 0.3864 - intent_output_loss: 0.6205 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 1.0000 - val_loss: 2.1516 - val_sentiment_output_loss: 0.7571 - val_intent_output_loss: 1.3944 - val_sentiment_output_accuracy: 0.5000 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.9696 - sentiment_output_loss: 0.3698 - intent_output_loss: 0.5999 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 1.0000 - val_loss: 2.1783 - val_sentiment_output_loss: 0.7549 - val_intent_output_loss: 1.4234 - val_sentiment_output_accuracy: 0.5000 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.9179 - sentiment_output_loss: 0.3318 - intent_output_loss: 0.5860 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 0.8333 - val_loss: 2.2069 - val_sentiment_output_loss: 0.7563 - val_intent_output_loss: 1.4506 - val_sentiment_output_accuracy: 0.5000 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 0.9364 - sentiment_output_loss: 0.3750 - intent_output_loss: 0.5614 - sentiment_output_accuracy: 0.8333 - intent_output_accuracy: 0.8333 - val_loss: 2.2306 - val_sentiment_output_loss: 0.7558 - val_intent_output_loss: 1.4748 - val_sentiment_output_accuracy: 0.5000 - val_intent_output_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.8127 - sentiment_output_loss: 0.3013 - intent_output_loss: 0.5113 - sentiment_output_accuracy: 1.0000 - intent_output_accuracy: 1.0000 - val_loss: 2.2554 - val_sentiment_output_loss: 0.7536 - val_intent_output_loss: 1.5018 - val_sentiment_output_accuracy: 0.5000 - val_intent_output_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.2554 - sentiment_output_loss: 0.7536 - intent_output_loss: 1.5018 - sentiment_output_accuracy: 0.5000 - intent_output_accuracy: 0.0000e+00\n",
            "Test Sentiment Accuracy: 0.50, Test Intent Accuracy: 0.00\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted Sentiment: Positive, Predicted Intent: purchase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqTgmLAw2bSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}